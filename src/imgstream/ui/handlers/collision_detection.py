"""
Collision detection utilities for filename conflicts.

This module provides comprehensive filename collision detection functionality
for the ImgStream application. It includes:

1. Basic collision detection for individual and batch files
2. Optimized batch processing for large file sets
3. Caching mechanism to improve performance
4. Fallback functionality for error recovery
5. Monitoring and logging integration

Key Features:
- Memory-efficient caching with TTL (Time To Live)
- Batch processing with configurable batch sizes
- Automatic fallback to safe mode on database errors
- Comprehensive logging and monitoring integration
- Thread-safe operations for concurrent access

Usage Examples:
    # Basic collision detection
    collisions = check_filename_collisions(user_id, ["photo1.jpg", "photo2.jpg"])

    # Optimized batch processing for large sets
    collisions = check_filename_collisions_optimized(user_id, large_file_list, batch_size=500)

    # With fallback for error recovery
    collisions, fallback_used = check_filename_collisions_with_fallback(user_id, filenames)

Performance Considerations:
- Cache TTL is configurable via COLLISION_CACHE_TTL_SECONDS environment variable
- Batch size should be adjusted based on database performance and memory constraints
- Cache statistics can be monitored via get_collision_cache_stats()
"""

from typing import Any
import time
import structlog
import os
from datetime import datetime, timedelta

from ...services.metadata import get_metadata_service, MetadataError
from ...logging_config import log_error

logger = structlog.get_logger(__name__)


class CollisionDetectionRecoveryError(Exception):
    """
    Raised when collision detection recovery fails.

    This exception is raised when both primary collision detection
    and fallback mechanisms fail, indicating a critical system issue
    that requires manual intervention.
    """

    pass


class CollisionCache:
    """
    Simple in-memory cache for collision detection results.

    This cache improves performance by storing recent collision detection
    results in memory with a configurable TTL (Time To Live). The cache
    is designed to be memory-efficient and thread-safe for concurrent access.

    Features:
    - TTL-based expiration to ensure data freshness
    - Automatic cleanup of expired entries
    - Memory usage monitoring and statistics
    - Thread-safe operations

    Cache Key Format:
        "{user_id}:{sorted_filename1}:{sorted_filename2}:..."

    Cache Value Format:
        (collision_results_dict, timestamp)
    """

    def __init__(self, ttl_seconds: int | None = None):
        """
        Initialize collision cache with configurable TTL.

        Args:
            ttl_seconds: Time to live for cache entries in seconds.
                        If None, uses COLLISION_CACHE_TTL_SECONDS environment variable
                        or defaults to 300 seconds (5 minutes).
        """
        if ttl_seconds is None:
            ttl_seconds = int(os.getenv("COLLISION_CACHE_TTL_SECONDS", 300))

        self.cache: dict[str, Any] = {}
        self.ttl_seconds = ttl_seconds
        self.max_entries = int(os.getenv("COLLISION_CACHE_MAX_ENTRIES", 10000))

    def _get_cache_key(self, user_id: str, filenames: list[str]) -> str:
        """
        Generate a unique cache key for collision detection.

        The cache key is generated by combining the user ID with sorted filenames
        to ensure consistent key generation regardless of filename order.

        Args:
            user_id: User identifier
            filenames: List of filenames to check for collisions

        Returns:
            Unique cache key string
        """
        sorted_filenames = sorted(filenames)
        return f"{user_id}:{':'.join(sorted_filenames)}"

    def get(self, user_id: str, filenames: list[str]) -> dict[str, Any] | None:
        """
        Get cached collision results if still valid.

        Checks if collision results for the given user and filenames are
        cached and still within the TTL period. Automatically removes
        expired entries during lookup.

        Args:
            user_id: User identifier
            filenames: List of filenames to check

        Returns:
            Cached collision results if valid, None otherwise
        """
        cache_key = self._get_cache_key(user_id, filenames)

        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]

            # Check if cache is still valid
            if datetime.now() - timestamp < timedelta(seconds=self.ttl_seconds):
                logger.debug(
                    "collision_cache_hit",
                    user_id=user_id,
                    cache_key=cache_key,
                    cached_results=len(cached_data),
                )
                return cached_data  # type: ignore[no-any-return]
            else:
                # Cache expired, remove it
                del self.cache[cache_key]
                logger.debug(
                    "collision_cache_expired",
                    user_id=user_id,
                    cache_key=cache_key,
                )
                logger.debug(
                    "collision_cache_expired",
                    user_id=user_id,
                    cache_key=cache_key,
                )

        return None

    def set(self, user_id: str, filenames: list[str], results: dict[str, Any]) -> None:
        """Cache collision detection results."""
        cache_key = self._get_cache_key(user_id, filenames)
        self.cache[cache_key] = (results, datetime.now())

        logger.debug(
            "collision_cache_set",
            user_id=user_id,
            cache_key=cache_key,
            cached_results=len(results),
        )

    def clear_user_cache(self, user_id: str) -> None:
        """Clear all cache entries for a specific user."""
        keys_to_remove = [key for key in self.cache.keys() if key.startswith(f"{user_id}:")]
        for key in keys_to_remove:
            del self.cache[key]

        logger.debug(
            "collision_cache_cleared_for_user",
            user_id=user_id,
            cleared_entries=len(keys_to_remove),
        )

    def clear_all(self) -> None:
        """Clear all cache entries."""
        cache_size = len(self.cache)
        self.cache.clear()

        logger.debug(
            "collision_cache_cleared_all",
            cleared_entries=cache_size,
        )

    def get_stats(self) -> dict[str, Any]:
        """Get cache statistics."""
        now = datetime.now()
        valid_entries = 0
        expired_entries = 0

        for _cached_data, timestamp in self.cache.values():
            if now - timestamp < timedelta(seconds=self.ttl_seconds):
                valid_entries += 1
            else:
                expired_entries += 1

        return {
            "total_entries": len(self.cache),
            "valid_entries": valid_entries,
            "expired_entries": expired_entries,
            "ttl_seconds": self.ttl_seconds,
        }


# Global cache instance
_collision_cache = CollisionCache()


def check_filename_collisions_with_retry(
    user_id: str, filenames: list[str], max_retries: int = 3, retry_delay: float = 1.0
) -> dict[str, dict[str, Any]]:
    """
    Check for filename collisions with retry logic and error recovery.

    Args:
        user_id: User identifier
        filenames: List of filenames to check for collisions
        max_retries: Maximum number of retry attempts
        retry_delay: Delay between retries in seconds

    Returns:
        dict: Dictionary mapping filename to collision info

    Raises:
        CollisionDetectionError: If collision detection fails after all retries
    """
    if not filenames:
        return {}

    last_error = None

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                logger.info(
                    "collision_detection_retry_attempt",
                    user_id=user_id,
                    attempt=attempt,
                    max_retries=max_retries,
                    delay=retry_delay,
                )
                time.sleep(retry_delay)

            return check_filename_collisions(user_id, filenames)

        except CollisionDetectionError as e:
            last_error = e
            if attempt < max_retries:
                logger.warning(
                    "collision_detection_failed_retrying",
                    user_id=user_id,
                    attempt=attempt,
                    error=str(e),
                    next_retry_in=retry_delay,
                )
                # Exponential backoff
                retry_delay *= 2
            else:
                logger.error(
                    "collision_detection_failed_all_retries",
                    user_id=user_id,
                    total_attempts=attempt + 1,
                    final_error=str(e),
                )
        except Exception as e:
            # For unexpected errors, don't retry
            logger.error(
                "collision_detection_unexpected_error",
                user_id=user_id,
                attempt=attempt,
                error=str(e),
                error_type=type(e).__name__,
            )
            raise CollisionDetectionError(f"Unexpected error during collision detection: {e}") from e

    # If we get here, all retries failed
    raise CollisionDetectionRecoveryError(
        f"Collision detection failed after {max_retries + 1} attempts. Last error: {last_error}"
    )


def check_filename_collisions(user_id: str, filenames: list[str], use_cache: bool = True) -> dict[str, dict[str, Any]]:
    """
    Check for filename collisions across multiple files for a user.

    Args:
        user_id: User identifier
        filenames: List of filenames to check for collisions
        use_cache: Whether to use caching for performance optimization

    Returns:
        dict: Dictionary mapping filename to collision info
              Format: {filename: collision_info} where collision_info is from MetadataService.check_filename_exists()

    Raises:
        CollisionDetectionError: If collision detection fails
    """
    if not filenames:
        return {}

    # Check cache first if enabled
    if use_cache:
        cached_results = _collision_cache.get(user_id, filenames)
        if cached_results is not None:
            logger.info(
                "collision_detection_cache_hit",
                user_id=user_id,
                total_files=len(filenames),
                cached_collisions=len(cached_results),
            )
            return cached_results

    failed_files: list[str] = []

    try:
        metadata_service = get_metadata_service(user_id)

        logger.info(
            "batch_collision_detection_started",
            user_id=user_id,
            total_files=len(filenames),
            filenames=filenames[:5],  # Log first 5 filenames for debugging
            cache_enabled=use_cache,
        )

        # Use individual collision detection for each filename
        collision_results = {}
        failed_files = []

        for filename in filenames:
            try:
                collision_info = metadata_service.check_filename_exists(filename)
                if collision_info:
                    collision_results[filename] = collision_info
                    logger.debug(
                        "collision_detected_in_individual_fallback",
                        user_id=user_id,
                        filename=filename,
                        existing_photo_id=collision_info["existing_photo"].id,
                    )

                    # Monitoring functionality removed for personal development use

            except MetadataError as e:
                failed_files.append(filename)
                logger.warning(
                    "collision_check_failed_for_file",
                    user_id=user_id,
                    filename=filename,
                    error=str(e),
                )
                # Continue with other files even if one fails
                continue
            except Exception as e:
                failed_files.append(filename)
                logger.error(
                    "collision_check_unexpected_error_for_file",
                    user_id=user_id,
                    filename=filename,
                    error=str(e),
                    error_type=type(e).__name__,
                )
                # Continue with other files
                continue

        # Log summary including failures
        logger.info(
            "batch_collision_detection_completed",
            user_id=user_id,
            total_files=len(filenames),
            collisions_found=len(collision_results),
            failed_files=len(failed_files),
            failed_filenames=failed_files[:5] if failed_files else [],
        )

        # Calculate failure rate
        failure_rate = len(failed_files) / len(filenames) if filenames else 0

        # Monitoring functionality removed for personal development use
        # Log batch metrics to collision monitor would be here

        # If too many files failed, consider it a system error
        if failure_rate > 0.5:  # More than 50% failed
            raise CollisionDetectionError(
                f"High failure rate in collision detection: {len(failed_files)}/{len(filenames)} files failed"
            )

        # Cache successful results if caching is enabled
        if use_cache and failure_rate < 0.1:  # Only cache if less than 10% failed
            _collision_cache.set(user_id, filenames, collision_results)

        return collision_results

    except Exception as e:
        if isinstance(e, CollisionDetectionError):
            raise

        log_error(
            e,
            {
                "operation": "check_filename_collisions",
                "user_id": user_id,
                "total_files": len(filenames),
                "failed_files": len(failed_files),
            },
        )
        raise CollisionDetectionError(f"Failed to check filename collisions: {e}") from e


def check_filename_collisions_with_fallback(
    user_id: str, filenames: list[str], enable_fallback: bool = True
) -> tuple[dict[str, dict[str, Any]], bool]:
    """
    Check for filename collisions with fallback to safe mode on failure.

    Args:
        user_id: User identifier
        filenames: List of filenames to check for collisions
        enable_fallback: Whether to enable fallback mode

    Returns:
        tuple: (collision_results, fallback_used)
               - collision_results: Dictionary mapping filename to collision info
               - fallback_used: True if fallback mode was used

    Raises:
        CollisionDetectionError: If both primary and fallback detection fail
    """
    try:
        # Try primary collision detection with retry
        collision_results = check_filename_collisions_with_retry(user_id, filenames)
        return collision_results, False

    except (CollisionDetectionError, CollisionDetectionRecoveryError) as e:
        if not enable_fallback:
            raise

        logger.warning(
            "collision_detection_failed_using_fallback",
            user_id=user_id,
            total_files=len(filenames),
            error=str(e),
        )

        try:
            # Fallback: Assume all files are potential collisions for safety
            fallback_results = _create_fallback_collision_results(user_id, filenames)

            logger.info(
                "collision_detection_fallback_completed",
                user_id=user_id,
                total_files=len(filenames),
                assumed_collisions=len(fallback_results),
            )

            return fallback_results, True

        except Exception as fallback_error:
            logger.error(
                "collision_detection_fallback_failed",
                user_id=user_id,
                primary_error=str(e),
                fallback_error=str(fallback_error),
            )
            raise CollisionDetectionError(
                f"Both primary and fallback collision detection failed. " f"Primary: {e}, Fallback: {fallback_error}"
            ) from e


def _create_fallback_collision_results(user_id: str, filenames: list[str]) -> dict[str, dict[str, Any]]:
    """
    Create fallback collision results that assume all files are potential collisions.

    This is a safety mechanism when collision detection fails completely.

    Args:
        user_id: User identifier
        filenames: List of filenames

    Returns:
        dict: Fallback collision results
    """
    from datetime import datetime

    fallback_results = {}

    for filename in filenames:
        # Create a minimal collision info structure for safety
        fallback_results[filename] = {
            "existing_photo": None,  # We don't have the actual photo
            "existing_file_info": {
                "file_size": 0,  # Unknown
                "photo_id": "unknown",
                "upload_date": datetime.now(),
                "created_at": None,
            },
            "collision_detected": True,
            "fallback_mode": True,
            "warning_message": "衝突検出に失敗したため、安全のため既存ファイルがあると仮定しています。",
        }

    return fallback_results


def process_collision_results(
    collision_results: dict[str, dict[str, Any]], user_decisions: dict[str, str] | None = None
) -> dict[str, Any]:
    """
    Process collision results and apply user decisions.

    Args:
        collision_results: Results from check_filename_collisions()
        user_decisions: Dictionary mapping filename to user decision ("overwrite" or "skip")

    Returns:
        dict: Processed collision information with decisions applied
              Format: {
                  "collisions": {filename: collision_info_with_decision},
                  "summary": {
                      "total_collisions": int,
                      "overwrite_count": int,
                      "skip_count": int,
                      "pending_count": int
                  }
              }
    """
    if not collision_results:
        return {
            "collisions": {},
            "summary": {
                "total_collisions": 0,
                "overwrite_count": 0,
                "skip_count": 0,
                "pending_count": 0,
            },
        }

    user_decisions = user_decisions or {}
    processed_collisions = {}
    overwrite_count = 0
    skip_count = 0
    pending_count = 0

    for filename, collision_info in collision_results.items():
        # Copy collision info and apply user decision
        processed_info = collision_info.copy()

        if filename in user_decisions:
            decision = user_decisions[filename]
            processed_info["user_decision"] = decision
            processed_info["warning_shown"] = True

            if decision == "overwrite":
                overwrite_count += 1
            elif decision == "skip":
                skip_count += 1
        else:
            # Keep as pending if no decision made
            processed_info["user_decision"] = "pending"
            pending_count += 1

        processed_collisions[filename] = processed_info

    summary = {
        "total_collisions": len(collision_results),
        "overwrite_count": overwrite_count,
        "skip_count": skip_count,
        "pending_count": pending_count,
    }

    logger.info(
        "collision_results_processed",
        total_collisions=summary["total_collisions"],
        overwrite_count=summary["overwrite_count"],
        skip_count=summary["skip_count"],
        pending_count=summary["pending_count"],
    )

    return {"collisions": processed_collisions, "summary": summary}


def filter_files_by_collision_decision(
    valid_files: list[dict[str, Any]], collision_results: dict[str, dict[str, Any]]
) -> dict[str, list[dict[str, Any]]]:
    """
    Filter files based on collision detection results and user decisions.

    Args:
        valid_files: List of validated file information dictionaries
        collision_results: Processed collision results with user decisions

    Returns:
        dict: Categorized files
              Format: {
                  "proceed_files": [file_info],  # Files to proceed with (new + overwrite)
                  "skip_files": [file_info],     # Files to skip due to collision
                  "collision_files": [file_info] # Files with collisions (for UI display)
              }
    """
    proceed_files = []
    skip_files = []
    collision_files = []

    for file_info in valid_files:
        filename = file_info["filename"]

        if filename in collision_results:
            collision_info = collision_results[filename]
            collision_files.append(
                {
                    **file_info,
                    "collision_info": collision_info,
                }
            )

            decision = collision_info.get("user_decision", "pending")
            if decision == "overwrite":
                proceed_files.append(
                    {
                        **file_info,
                        "is_overwrite": True,
                        "collision_info": collision_info,
                    }
                )
            elif decision == "skip":
                skip_files.append(
                    {
                        **file_info,
                        "collision_info": collision_info,
                    }
                )
            # pending decisions are not included in proceed_files
        else:
            # No collision, proceed as new file
            proceed_files.append(
                {
                    **file_info,
                    "is_overwrite": False,
                }
            )

    logger.info(
        "files_filtered_by_collision_decision",
        total_files=len(valid_files),
        proceed_files=len(proceed_files),
        skip_files=len(skip_files),
        collision_files=len(collision_files),
    )

    return {
        "proceed_files": proceed_files,
        "skip_files": skip_files,
        "collision_files": collision_files,
    }


def get_collision_summary_message(collision_summary: dict[str, int]) -> str:
    """
    Generate a user-friendly summary message for collision results.

    Args:
        collision_summary: Summary from process_collision_results()

    Returns:
        str: Human-readable summary message
    """
    total = collision_summary["total_collisions"]

    if total == 0:
        return "ファイル名の衝突は検出されませんでした。"

    overwrite = collision_summary["overwrite_count"]
    skip = collision_summary["skip_count"]
    pending = collision_summary["pending_count"]

    parts = []
    if total == 1:
        parts.append("1件のファイル名衝突が検出されました")
    else:
        parts.append(f"{total}件のファイル名衝突が検出されました")

    if overwrite > 0:
        parts.append(f"{overwrite}件を上書き")
    if skip > 0:
        parts.append(f"{skip}件をスキップ")
    if pending > 0:
        parts.append(f"{pending}件が決定待ち")

    return "。".join(parts) + "。"


class CollisionDetectionError(Exception):
    """Exception raised when collision detection fails."""

    def __init__(self, message: str, original_error: Exception | None = None):
        super().__init__(message)
        self.original_error = original_error


def get_collision_cache_stats() -> dict[str, Any]:
    """
    Get collision detection cache statistics.

    Returns:
        dict: Cache statistics including hit rate, size, etc.
    """
    return _collision_cache.get_stats()


def clear_collision_cache(user_id: str | None = None) -> None:
    """
    Clear collision detection cache.

    Args:
        user_id: If provided, clear only cache for this user. If None, clear all cache.
    """
    if user_id:
        _collision_cache.clear_user_cache(user_id)
    else:
        _collision_cache.clear_all()


def monitor_collision_detection_performance(func):
    """
    Decorator to monitor collision detection performance.

    Args:
        func: Function to monitor

    Returns:
        Wrapped function with performance monitoring
    """
    import functools
    from time import perf_counter

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = perf_counter()

        try:
            result = func(*args, **kwargs)
            end_time = perf_counter()
            duration = end_time - start_time

            # Extract user_id and file count for logging
            user_id = args[0] if args else "unknown"
            filenames = args[1] if len(args) > 1 else []
            file_count = len(filenames) if isinstance(filenames, list) else 0

            logger.info(
                "collision_detection_performance",
                function=func.__name__,
                user_id=user_id,
                file_count=file_count,
                duration_seconds=duration,
                files_per_second=file_count / duration if duration > 0 else 0,
                success=True,
            )

            return result

        except Exception as e:
            end_time = perf_counter()
            duration = end_time - start_time

            user_id = args[0] if args else "unknown"
            filenames = args[1] if len(args) > 1 else []
            file_count = len(filenames) if isinstance(filenames, list) else 0

            logger.error(
                "collision_detection_performance",
                function=func.__name__,
                user_id=user_id,
                file_count=file_count,
                duration_seconds=duration,
                success=False,
                error=str(e),
                error_type=type(e).__name__,
            )

            raise

    return wrapper


# Apply performance monitoring to key functions
check_filename_collisions = monitor_collision_detection_performance(check_filename_collisions)
check_filename_collisions_with_retry = monitor_collision_detection_performance(check_filename_collisions_with_retry)
check_filename_collisions_with_fallback = monitor_collision_detection_performance(
    check_filename_collisions_with_fallback
)


def optimize_collision_detection_query(filenames: list[str], batch_size: int = 100) -> list[list[str]]:
    """
    Optimize collision detection by batching filenames into optimal query sizes.

    Args:
        filenames: List of filenames to check
        batch_size: Maximum number of filenames per batch

    Returns:
        List of filename batches
    """
    if not filenames:
        return []

    # Split filenames into batches for optimal query performance
    batches = []
    for i in range(0, len(filenames), batch_size):
        batch = filenames[i : i + batch_size]
        batches.append(batch)

    logger.debug(
        "collision_detection_query_optimization",
        total_files=len(filenames),
        batch_count=len(batches),
        batch_size=batch_size,
    )

    return batches


def check_filename_collisions_optimized(
    user_id: str, filenames: list[str], batch_size: int = 100
) -> dict[str, dict[str, Any]]:
    """
    Optimized collision detection with batching and caching.

    Args:
        user_id: User identifier
        filenames: List of filenames to check for collisions
        batch_size: Maximum number of filenames per batch query

    Returns:
        dict: Dictionary mapping filename to collision info

    Raises:
        CollisionDetectionError: If collision detection fails
    """
    if not filenames:
        return {}

    # For small lists, use regular collision detection
    if len(filenames) <= batch_size:
        return check_filename_collisions(user_id, filenames, use_cache=True)

    # For large lists, process in batches
    batches = optimize_collision_detection_query(filenames, batch_size)
    all_collision_results = {}

    logger.info(
        "optimized_collision_detection_started",
        user_id=user_id,
        total_files=len(filenames),
        batch_count=len(batches),
        batch_size=batch_size,
    )

    for i, batch in enumerate(batches):
        try:
            batch_results = check_filename_collisions(user_id, batch, use_cache=True)
            all_collision_results.update(batch_results)

            logger.debug(
                "collision_detection_batch_completed",
                user_id=user_id,
                batch_index=i + 1,
                batch_size=len(batch),
                batch_collisions=len(batch_results),
            )

        except Exception as e:
            logger.error(
                "collision_detection_batch_failed",
                user_id=user_id,
                batch_index=i + 1,
                batch_size=len(batch),
                error=str(e),
            )
            # Continue with other batches
            continue

    logger.info(
        "optimized_collision_detection_completed",
        user_id=user_id,
        total_files=len(filenames),
        total_collisions=len(all_collision_results),
        batch_count=len(batches),
    )

    return all_collision_results
